{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce4dc60-3508-4cbe-95eb-3309a3b07a82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e008f7db735a42f6b508095746f1c173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'LABEL_0', 'score': 0.540632963180542}\n"
     ]
    }
   ],
   "source": [
    "# Importing HuggingFace Transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initializing a text classification pipeline with a pre-trained BERT model\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Using the pipeline to classify a sentence\n",
    "result = classifier(\"I love using HuggingFace Transformers\")[0]\n",
    "\n",
    "# Output: {'label': 'POSITIVE', 'score': 0.9998656511306763}\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "439ddcb5-2e1b-48c2-b204-522ca162bcb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2291,  0.0135, -0.1188,  ..., -0.2613, -0.0115,  0.6596],\n",
      "         [-0.0107,  0.0749,  0.6104,  ..., -0.1752,  0.3824, -0.0338],\n",
      "         [-0.7685,  0.8493,  0.3917,  ..., -1.2588, -0.3544,  0.0577],\n",
      "         ...,\n",
      "         [ 0.2830, -0.0994,  1.1351,  ..., -0.4314,  0.6670, -0.0023],\n",
      "         [-0.5194, -0.1928, -0.3060,  ...,  0.6108, -0.3512,  0.1036],\n",
      "         [ 0.7359,  0.3968, -0.0619,  ...,  0.4613, -0.4899, -0.3350]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary classes from HuggingFace Transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Initializing the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenizing input text\n",
    "inputs = tokenizer(\"Hello, HuggingFace!\", return_tensors=\"pt\")\n",
    "\n",
    "# Feeding the inputs to the model\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Output: tensor containing the hidden states of the last layer of the model\n",
    "print(outputs.last_hidden_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f3e87-8011-4420-bf4b-87b10486d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary classes from HuggingFace Transformers\n",
    "from transformers import BertModel, GPT2Model, RobertaModel, DistilBertModel, T5Model\n",
    "\n",
    "# Initializing the models\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "t5 = T5Model.from_pretrained('t5-base')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
