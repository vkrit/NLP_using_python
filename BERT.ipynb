{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23505e3e-dd54-4db4-9c70-3920873cb2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?']\n",
    "\n",
    "# Create the BoW model\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Summarize\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray()) \n",
    "\n",
    "# Output\n",
    "# ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "# [[0 1 1 1 0 0 1 0 1]\n",
    "#  [0 2 0 1 0 1 1 0 1]\n",
    "#  [1 0 0 1 1 0 1 1 1]\n",
    "#  [0 1 1 1 0 0 1 0 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb400269-ae3f-4f80-a1a7-a26e3d846bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39315931 0.458803   0.39315931 0.         0.         0.\n",
      "  0.         0.39315931 0.         0.         0.         0.\n",
      "  0.         0.458803   0.33952451 0.         0.         0.        ]\n",
      " [0.30999542 0.36175368 0.30999542 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.52252953 0.\n",
      "  0.         0.36175368 0.         0.52252953 0.         0.        ]\n",
      " [0.         0.         0.         0.29665213 0.         0.29665213\n",
      "  0.29665213 0.         0.42849459 0.29665213 0.         0.42849459\n",
      "  0.29665213 0.         0.43905847 0.         0.         0.        ]\n",
      " [0.25164011 0.         0.25164011 0.2936551  0.         0.2936551\n",
      "  0.2936551  0.50328023 0.         0.2936551  0.         0.\n",
      "  0.2936551  0.         0.43462273 0.         0.         0.        ]\n",
      " [0.182579   0.21306323 0.182579   0.         0.         0.\n",
      "  0.         0.365158   0.         0.         0.         0.\n",
      "  0.         0.42612646 0.31534314 0.         0.3077559  0.61551179]\n",
      " [0.         0.         0.         0.2861327  0.41329997 0.2861327\n",
      "  0.2861327  0.49038798 0.         0.2861327  0.         0.\n",
      "  0.2861327  0.         0.42348924 0.         0.         0.        ]]\n",
      "['and' 'beautiful' 'blue' 'brown' 'but' 'dog' 'fox' 'is' 'jumps' 'lazy'\n",
      " 'love' 'over' 'quick' 'sky' 'the' 'this' 'today' 'very']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "    'The sky is blue and beautiful.',\n",
    "    'Love this blue and beautiful sky!',\n",
    "    'The quick brown fox jumps over the lazy dog.',\n",
    "    'The brown fox is quick and the blue dog is lazy.',\n",
    "    'The sky is very blue and the sky is very beautiful today.',\n",
    "    'The dog is lazy but the brown fox is quick.'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# print the tf-idf values\n",
    "print(X.toarray()) \n",
    "\n",
    "# print the feature names\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5f0846-4ff6-41da-8d61-a99dd63cf696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d4743d6-6f4b-4197-a2a6-a9c305914a28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Masking tensor([[  101,  8891,  4329,  5084,  2510,  5502,  2532, 15049,  2239,  2038,\n",
      "          8027, 23115, 15719,  2145,  3268,   102]])\n",
      "After Masking tensor([[  101,  8891,  4329,  5084,  2510,  5502,  2532, 15049,  2239,  2038,\n",
      "          8027, 23115, 15719,  2145,  3268,   102]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "text = (\"Napoleon revolutionised military organisation\"\n",
    "\"Napoleon has legacy\"\n",
    "\"Legacy still lives\"\n",
    ")\n",
    "rep = tokenizer(text, return_tensors = \"pt\")\n",
    "print(\"Before Masking\",rep.input_ids)\n",
    "rand = torch.rand(rep.input_ids.shape)\n",
    "mask_arr = (rand < 0.15) * (rep.input_ids != 101) * (rep.input_ids != 102)\n",
    "selection = torch.flatten(mask_arr[0].nonzero()).tolist()\n",
    "rep.input_ids[0, selection] = 103\n",
    "after_masking = rep.input_ids\n",
    "print(\"After Masking\", after_masking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d821a5-d8a1-49e8-acb2-c7283d7ff0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
